{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9j_R9YlKV9oV"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from sklearn.metrics import (recall_score,accuracy_score, \n",
        "precision_score, confusion_matrix)\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "# from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "# from sklearn.neighbors import KNeighborsClassifier\n",
        "import seaborn as sb\n",
        "from mlxtend.plotting import plot_learning_curves\n",
        "from mlxtend.data import mnist_data\n",
        "from mlxtend.preprocessing import shuffle_arrays_unison\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "import nltk\n",
        "nltk.download('all')\n",
        "import string\n",
        "from nltk.corpus import stopwords"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/sample_data/Reviews_1.csv')"
      ],
      "metadata": {
        "id": "toBeeUVEWIki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df=df.drop_duplicates(subset={\"Text\"}, keep='first', inplace=False)"
      ],
      "metadata": {
        "id": "frQrw-PAWL1c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cachedStopWords = stopwords.words(\"english\")\n",
        "sentences = []\n",
        "for values in df['Text']:\n",
        "  text = values\n",
        "  text = ''.join(i for i in text if not i.isdigit())\n",
        "  text = text.lower()\n",
        "  text = re.sub(r\"http\\S|www\\S+|https\\S+\",\"\",text,flags=re.MULTILINE)\n",
        "  text = text.translate(str.maketrans(\"\",\"\",string.punctuation))\n",
        "  text = ' '.join([word for word in text.split() if word not in cachedStopWords])\n",
        "  sentences.append(text)\n",
        "  del text\n",
        "\n",
        "sentences_df = pd.DataFrame(sentences, columns=['reviews'])"
      ],
      "metadata": {
        "id": "PVLahU5xWVPM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# total number of unique words\n",
        "words = set()\n",
        "for values in sentences_df['reviews']:\n",
        "  text = values\n",
        "  tmp_list = text.split()\n",
        "  words.update(tmp_list)\n",
        "\n",
        "num_words = len(words)\n",
        "print(len(words))\n",
        "print(num_words)"
      ],
      "metadata": {
        "id": "ts0IOZgcWW6e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_sentences,testing_sentences,training_labels,testing_labels = train_test_split(sentences_df['reviews'].values,df['Score'].values,test_size=.20, random_state=0)     \n",
        "print(training_sentences.shape)\n",
        "print(testing_sentences.shape)"
      ],
      "metadata": {
        "id": "lUTN276YWbPZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tokenizer and glove embedding\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "tokenizer = Tokenizer(num_words = num_words, oov_token = \"<OOV>\")\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "# word_index = tokenizer.word_index\n",
        "# print(len(word_index))\n",
        "# del word_index\n",
        "tokenizer.fit_on_texts(testing_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "print(len(word_index))\n",
        "# glove_train = tokenizer.word_index\n",
        "# tokenizer.fit_on_texts(testing_sentences)\n",
        "# glove_test = tokenizer.word_index\n",
        "\n",
        "traning_sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "glove_train = pad_sequences(traning_sequences, maxlen=500)\n",
        "testing_sequences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "glove_test = pad_sequences(testing_sequences, maxlen=500)\n",
        "\n",
        "# glove_train = np.array(glove_train)\n",
        "# training_labels = np.array(training_labels)\n",
        "# glove_test = np.array(glove_test)\n",
        "# testing_labels = np.array(testing_labels)"
      ],
      "metadata": {
        "id": "C7CFQMAtWncp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict = {}\n",
        "with open('/content/sample_data/glove.twitter.27B.25d.txt', 'r') as f:\n",
        "  for line in f:\n",
        "    values = line.split()\n",
        "    word = values[0]\n",
        "    vectors = np.asarray(values[1:], 'float32')\n",
        "    embedding_dict[word] = vectors\n",
        "f.close()"
      ],
      "metadata": {
        "id": "qvaiAoMmWyQx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_words += 1\n",
        "embedding_matrix = np.zeros((num_words, 25))\n",
        "\n",
        "for word, i in word_index.items():\n",
        "  if i<num_words:\n",
        "    emb_vec = embedding_dict.get(word)\n",
        "    if emb_vec is not None:\n",
        "      embedding_matrix[i] = emb_vec"
      ],
      "metadata": {
        "id": "wuN6CXZ3Wz5t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_matrix"
      ],
      "metadata": {
        "id": "_73ZRjP1W5sk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "word_index['separate']"
      ],
      "metadata": {
        "id": "0SzEeHPLW7mR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dict.get('separate')"
      ],
      "metadata": {
        "id": "PmapdrIfW_u6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#LSTM with glove embedding\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Embedding, LSTM, Dropout\n",
        "from keras.initializers import Constant\n",
        "from keras.optimizers import Adam\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(num_words, 25, embeddings_initializer = Constant(embedding_matrix), \n",
        "                    input_length=500, trainable = False))\n",
        "model.add(LSTM(100, dropout=0.1))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "optimizer = Adam(learning_rate = 3e-4)\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "print(model.summary())"
      ],
      "metadata": {
        "id": "RH-KzgUyXNt8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.utils.np_utils import to_categorical\n",
        "from keras.callbacks import EarlyStopping\n",
        "from keras.layers import Dropout\n",
        "epochs = 5\n",
        "batch_size = 1000\n",
        "\n",
        "history = model.fit(glove_train, training_labels, epochs=epochs,validation_data=(glove_test,testing_labels))\n"
      ],
      "metadata": {
        "id": "0UF5mOP7XUl9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}